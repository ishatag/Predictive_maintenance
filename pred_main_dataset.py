# -*- coding: utf-8 -*-
"""Pred_main_dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HepnDwAs3VvBgotqSf7s1Pp9YFUwQLfT
"""

#Goal: Develop a machine learning model to predict machine failure (classification), identify specific failure types, and offer actionable maintenance strategies to reduce downtime.

#import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('predictive_maintenance.csv')

"""EXPLORATORY DATA ANALYSIS- EDA"""

df.head()

#create a plot
import matplotlib.pyplot as plt
import numpy as np
x = np.linspace(0, 10, 100)
y = np.sin(x)
plt.plot(x, y)

#pairplot
sns.pairplot(df)

#plot figure...subpot, histplot
plt.figure(figsize=(12, 6))
# Adjust the subplot grid to accommodate all columns (2 rows, 5 columns)
num_cols = len(df.columns)
num_rows = (num_cols + 4) // 5  # Calculate rows needed, ensuring at least 2

for i, col in enumerate(df.columns, 1):
    plt.subplot(num_rows, 5, i) # Changed subplot layout
    sns.histplot(df[col], kde=True)

plt.tight_layout()  # Adjust spacing to prevent overlap
plt.show()

df.info()

df.isnull().sum()

"""Shows there is no missing values in dataset

"""

df.shape

df.describe()

df.info()

df.apply(lambda x: x.nunique())

"""The previous code line meaning: "Check each column in the dataset and count how many different values are there in it."

It‚Äôs often used to quickly explore a dataset and understand:

1. Which columns are numeric vs categorical

2. If some columns have the same value in every row

3. How much variation exists in your data

#A lambda is just a quick, mini function you can write in one line ‚Äî without giving it a name.
"""

df.iloc[:,-1].unique()

"""What is iloc?
iloc stands for "integer-location based indexing".

It's used in Pandas to select rows and columns by number, not by name.

Breakdown of df.iloc[:, -1]:
df is your DataFrame.
: means "all rows".

-1 means "last column" (Python allows negative indexing; -1 is the last item).

>>>So, df.iloc[:, -1] means:

‚ÄúGive me the last column for all the rows.‚Äù

.unique()
This is a Pandas method that shows all the unique values in a column (or series).

It removes duplicates and gives you just the distinct values.
"""



"""Cannot use both Target and Failure Type together in the same predictive model because it will cause what‚Äôs called data leakage (specifically,** Label leakage**

**Problem (Label Leakage)**
If you include both columns as features or try to predict one using the other:

The model will "cheat" and learn to just copy the answer.

You will get unrealistically high accuracy during training.

But in the real world, your model will fail because it learned from information it shouldn‚Äôt have had.

SOLUTION:

In case of binary classification, use only the Target column. Drop or ignore Failure Type.

In case of multiclass classification, use only Failure Type (and remove or ignore Target).

Never include both as target or feature in the same model.
"""

#binary classification
df.drop(df.columns[-1],axis=1,inplace=True)

#Explore the Target Variable
df['Target'].value_counts().plot(kind='bar', title='Target Distribution')
plt.show()

"""The plot above is imbalanced. It means classes in the target variable are not equally represented.

Class 0 (No failure) is dominant

Class 1 (Failure) is rare

**Why Is Imbalanced Data a Problem?**
Misleading Accuracy
If 95% of samples are class 0, a model predicting "always 0" can get 95% accuracy ‚Äî but never detects actual failures.

Bias Toward Majority Class
Machine learning algorithms tend to favor the majority class, leading to poor performance on minority (important!) class.

Low Recall for Critical Events
Especially in your project (predicting failure), failing to catch true failures (false negatives) is risky.

**What Causes Imbalanced Data?**
Rare events (like mechanical failures, fraud, disease)

Sampling biases

Natural distribution of events (some things really are rare)

**3 Strategies to deal with it**
1. SMOTE (Synthetic Minority Oversampling Technique)
2. Class Weights in the Model
3. Use Better Metrics



"""

#Encode Categorical Columns
# Check for object (categorical) columns
df.select_dtypes(include='object').columns

# Apply label encoding or one-hot encoding
df = pd.get_dummies(df, drop_first=True)  # fast and simple

"""**LabelEncoder** is that computer.
It turns text labels (categories) into numbers, so machine learning models can process them.


"""

#  Split Data into Features and Target
X = df.drop('Target', axis=1)
y = df['Target']

#now use 1. SMOT
from imblearn.over_sampling import SMOTE

sm = SMOTE(random_state=42)
X_resampled, y_resampled = sm.fit_resample(X, y)

# 2. Class Weights in the Model
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

clf = RandomForestClassifier(class_weight='balanced')

# Train-Test Split

X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Scale features
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#Train a Model >> Random Forest
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

clf = RandomForestClassifier()
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

# 3.Evaluate the Model
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

#Recheck the perfect scores----
df.corr()['Target'].sort_values(ascending=False)

"""The correlation shows that the features are not strongly correlated to the Target variable."""

#Train a model>> Logistics Regression
from sklearn.linear_model import LogisticRegression

clf = LogisticRegression()
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

#Evaluate the model on Logistics Regression
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Convert all column names to string and remove brackets or special characters
X_train.columns = X_train.columns.astype(str).str.replace('[<>\[\]]', '', regex=True)
X_test.columns = X_test.columns.astype(str).str.replace('[<>\[\]]', '', regex=True)

#train a model>> XGBoost
from xgboost import XGBClassifier

clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

#Evaluate a model on XGBoost
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=[0,1], yticklabels=[0,1])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

"""This confusion matrix shows an exceptionally strong model ‚Äî over 99% accuracy, with very high precision and recall. The model only missed 10 churners and falsely flagged 25 non-churners, meaning it's both accurate and business-safe. I'd trust this model in production

**Multiclass Classification ‚Äî Model 2**
"What type of failure will it be?" (on failures only)
"""

df.head()
#this is original data---I want to restart again

#import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')


# reload dataset> as df1 for predictive_maintenance.csv
df1 = pd.read_csv('predictive_maintenance_1.csv')
df1.head()

df1.shape

df1.columns
#df.columns

#Filter Only Failures for Multiclass Classification
failures_only = df1[df1['Target'] == 1].copy()

#Drop Target and UDI Columns & Prepare Features
# Drop columns not needed
failures_only.drop(['Target', 'UDI'], axis=1, inplace=True)

# Split features and target
X = failures_only.drop('Failure Type', axis=1)
y = failures_only['Failure Type']

"""Target and UDI are dropped to avoid data leakage, noise, and misleading the model.
This ensures your features are clean, meaningful, and genuinely useful for classification.
"""

#Encode Target and Preprocess Features
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Encode target
le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Encode categorical feature (Product ID)
X = pd.get_dummies(X, drop_first=True)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42)

print(X_train.columns.tolist())

"""Check correlation between each one-hot encoded product column and the failure type (Failure Type)."""

import seaborn as sns
import matplotlib.pyplot as plt

# Create a temp df with product columns and the target
product_cols = [col for col in df1.columns if 'Product ID_' in col]
corr_df = df1[product_cols + ['Failure Type']]

# Convert Failure Type to numeric temporarily
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
corr_df['Failure Type'] = le.fit_transform(corr_df['Failure Type'])

# Plot heatmap
plt.figure(figsize=(12, 6))
sns.heatmap(corr_df.corr(), annot=True, cmap='coolwarm')
plt.title("Correlation between Product IDs and Failure Type")
plt.show()

"""As there is High correlation (near ¬±1) ‚Üí I will be dropping those product ID columns (they may leak target info)."""

# Identify all Product ID columns
product_cols = [col for col in df1.columns if 'Product ID_' in col]

# Drop them from the dataset
df1.drop(columns=product_cols, inplace=True)

# Confirm
df1.head()

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df1['Failure Type'] = le.fit_transform(df1['Failure Type'])  # Now numerical labels

#Features and target
X = df1.drop(columns=['Failure Type','Target', 'UDI'])  # Dropping unused or ID columns
y = df1['Failure Type']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# ----> Apply one-hot encoding for object type columns
# Get a list of object type columns
object_cols = X.select_dtypes(include=['object']).columns

# Apply one-hot encoding to object type columns
X_train = pd.get_dummies(X_train, columns=object_cols, drop_first=True)

# Clean column names
X = X.rename(columns=lambda col: col.replace('[', '_').replace(']', '_').replace('<', '_').replace('>', '_'))

# Drop non-useful object columns
X = X.drop(columns=['Product ID'], errors='ignore')

# Encode 'Type' if needed
from sklearn.preprocessing import LabelEncoder
if 'Type' in X.columns:
    le_type = LabelEncoder()
    X['Type'] = le_type.fit_transform(X['Type'])

# Train/test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

# Train XGBoost
from xgboost import XGBClassifier
model = XGBClassifier(objective='multi:softmax', num_class=len(np.unique(y)), eval_metric='mlogloss')
model.fit(X_train, y_train)

from xgboost import plot_importance
plot_importance(model)
plt.show()

#find out Feature importance of Failure Type and create a plot in > y = df1['Failure Type']

from imblearn.over_sampling import SMOTE

sm = SMOTE(random_state=42)
X_resampled, y_resampled = sm.fit_resample(X_train, y_train)

#define classification report
from sklearn.metrics import classification_report, confusion_matrix

# Predict on the test set
y_pred = model.predict(X_test)

# Evaluate the model
print(classification_report(y_test, y_pred, target_names=le.classes_))
print(confusion_matrix(y_test, y_pred))

"""‚úÖ **No Failure** is predicted very well (Precision: 0.99, Recall: 1.00) ‚Äî **great accuracy.**

‚ö†Ô∏è **Tool Wear Failure & Random Failures have 0 scores across the board ‚Äî the model fails to identify them at all.**

‚ö†Ô∏è These two failure types have very low support (5 and 14 instances) ‚Äî this is a classic case of class imbalance.

‚úÖ Other failure types like Heat Dissipation, Overstrain, and Power Failure are predicted decently but still need improvement.


"""

#Retrain and re-evaluate the model to check if performance improves.
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42, stratify=y_resampled)

#Re-evaluate the model
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred, target_names=le.classes_))
print(confusion_matrix(y_test, y_pred))

"""| Failure Type         | High Precision? | High Recall? | Reliable Prediction? |
|----------------------|-----------------|--------------|-----------------------|
| Heat Dissipation     | ‚úÖ              | ‚úÖ           | ‚úÖ Yes                |
| Overstrain           | ‚úÖ              | ‚úÖ           | ‚úÖ Yes                |
| Power Failure        | ‚úÖ              | ‚úÖ           | ‚úÖ Yes                |
| Tool Wear            | ‚úÖ              | ‚ùå           | ‚ö†Ô∏è No ‚Äî misses real failures |
| Random Failure       | ‚úÖ              | ‚ùå‚ùå         | ‚ùå Not reliable       |
| No Failure           | ‚ùå              | ‚úÖ           | ‚ö†Ô∏è Too many false negatives |

‚ÄúPrecision tells me how trustworthy each prediction is, while recall tells me how complete the model‚Äôs detection is. For rare but critical failures like Tool Wear and Random Failures, high recall is more important ‚Äî because missing them could cause unplanned downtime. My model is strong on Heat Dissipation and Power Failure, but needs better recall on rarer classes. I‚Äôd improve this using class-specific features, SMOTE, or custom loss functions.‚Äù

**BETTER LOST FUNCTIONS:**                                     A loss function tells your model: ‚ÄúHow wrong are you right now?‚Äù The model learns by minimizing that "wrongness" ‚Äî just like learning from mistakes.
"""

# Better Loss Functions
#For Multiclass: You can‚Äôt directly use focal_loss in XGBoost yet,
#Therefore, use  class weights manually in multiclass XGBoost

from sklearn.utils.class_weight import compute_class_weight
import numpy as np

# Get weights for each class
weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)

# Apply sample weights to training
sample_weights = np.array([weights[i] for i in y_train])

model = XGBClassifier(objective='multi:softmax', num_class=len(np.unique(y_train)))
model.fit(X_train, y_train, sample_weight=sample_weights)

#Evaluate the model again
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred, target_names=le.classes_))
print(confusion_matrix(y_test, y_pred))

"""** the Evaluation Metrics Tell Us**:              

| Class                     | Precision | Recall | F1-score | What it Means |
|---------------------------|-----------|--------|----------|----------------|
| **Heat Dissipation**      | 1.00      | 1.00   | 1.00     | Perfectly predicted üî• |
| **No Failure**            | 1.00      | 0.96   | 0.98     | Slightly over-predicting failure |
| **Overstrain**            | 1.00      | 1.00   | 1.00     | Perfect ‚úÖ |
| **Power Failure**         | 1.00      | 1.00   | 1.00     | Perfect ‚ö° |
| **Random Failures**       | 0.98      | 1.00   | 0.99     | Huge improvement from earlier ‚ùó |
| **Tool Wear Failure**     | 0.99      | 1.00   | 0.99     | Also drastically better üëè |**bold text**
"""